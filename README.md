# Classifying Social Media Posts by Racism and Hate Speech

A robust, script-driven pipeline for binary hate-speech detection using **5-fold cross-validation** and **advanced feature engineering**. It ingests CSV dumps from multiple social platforms, applies Random Forest-based feature selection to TF-IDF features, and trains logistic-regression classifiers with statistically reliable performance estimates.

## ğŸ“Š Datasets

The project uses three datasets that should be present in the `data/` folder:

- `reddit_dataset.csv` - Reddit comments with `Comment` and `Hateful` columns
- `4chan_dataset.csv` - 4chan posts with `Comment` and `Hateful` columns
- `twitter_dataset.csv` - Twitter posts with `Comment` and `Hateful` columns

Each dataset should have:
- `Comment` column containing the text
- `Hateful` column with binary labels (0/1 or True/False)
- Additional columns (mentions, timestamps, etc.) are ignored by default but preserved if you extend the extractor.

**Note:** Ensure these three CSV files are present in the `data/` folder before running any scripts.

## ğŸš€ Quick Start

```bash
cd Classifying-Social-Media-Posts-by-Racism-and-Hate-Speech
python -m venv .venv && source .venv/bin/activate      # or use your env manager
pip install -r requirements.txt

# Generate hate speech lexicon from curated sources
# NOTE: This script only needs to be run ONCE (the first time). It creates data/hate_lexicon.txt
# which will be reused by all subsequent training runs. The lexicon is a fixed curated list.
python scripts/download_hate_lexicon.py

# Train with 5-fold cross-validation on all three datasets
python scripts/train.py

# Optional: reproduce the feature-group ablation study
python scripts/feature_group_comparison.py
```

The `download_hate_lexicon.py` script should only be run **once** (the first time you set up the project). It generates a fixed curated hate speech lexicon and saves it to `data/hate_lexicon.txt`. This file will be reused by all subsequent training and evaluation scripts, so you don't need to run the script again unless you want to update the lexicon.

Both scripts assume the working directory is the project root so the relative `data/...` paths resolve correctly.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data/                        # Dataset CSV files and hate lexicon
â”‚   â”œâ”€â”€ reddit_dataset.csv
â”‚   â”œâ”€â”€ 4chan_dataset.csv
â”‚   â”œâ”€â”€ twitter_dataset.csv
â”‚   â””â”€â”€ hate_lexicon.txt         # Generated by download_hate_lexicon.py (fixed curated lexicon)
â”œâ”€â”€ runs/                        # Auto-created experiment folders
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Main multi-dataset training pipeline
â”‚   â”œâ”€â”€ feature_group_comparison.py
â”‚   â””â”€â”€ download_hate_lexicon.py # Generates hate lexicon from curated sources
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ features.py              # TextFeatureExtractor and dataset loader helpers
â”‚   â””â”€â”€ utils.py                 # Utilities: run directory, save/load, seed setting
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ Training Pipeline (`scripts/train.py`)

### Methodology Overview

1. **Load datasets** via `src.features.load_dataset` from:
   - `data/reddit_dataset.csv`
   - `data/4chan_dataset.csv`
   - `data/twitter_dataset.csv`
2. **5-Fold Stratified Cross-Validation**:
   - For each dataset separately (Reddit, 4chan, Twitter), extract features from the full dataset once using `TextFeatureExtractor`.
   - Create 5 stratified folds by splitting the feature matrix by indices (80% train, 20% test per fold).
   - Store both the full feature matrix and per-fold splits for different training regimes.
3. **Feature Engineering (5 groups)**:
   - **Lexical**: length and style statistics (14 features)
     (`num_words`, `avg_word_len`, `uppercase_ratio`, `exclamation_ratio`, `question_ratio`, `period_ratio`, `punctuation_count`, `multiple_count`, `char_count`, `repeated_chars`, `unique_words`, `word_repetition`, `longest_word_length`, `shortest_word_length`, `language_complexity_index`)
   - **Hate-Lexicon**: token overlap with expanded `data/hate_lexicon.txt` 
     (fixed curated lexicon)
     (`hate_word_count`, `hate_word_ratio`, `contains_hate_word`)
   - **TF-IDF**: n-gram TF-IDF features with Random Forest-based selection  
     (up to 4000 â†’ 1000 selected `tfidf_*` features)
   - **Sentiment**: TextBlob polarity/subjectivity and derived flags  
     (polarity, subjectivity, negative/neutral/positive, extreme sentiment, high/low subjectivity)
   - **Embedding-based**: sentence embeddings and semantic stats  
     (384D MiniLM embeddings, POS ratios, pronoun ratios, hate-lexicon similarity, distance to neutral)
4. **Logistic Regression Classifier**:
   - Binary classifier with `class_weight='balanced'`, `solver='saga'`, `max_iter=10000`.
5. **Evaluation Regimes**:
   - **Per-dataset 5-fold CV**: train on 800 samples per fold and test on 200 samples per fold within the same platform; report mean Â± std per dataset.
   - **Per-dataset cross-dataset**: train on 100% of one dataset (1000 samples), test on all 5 folds of each other dataset; aggregate metrics across folds.
   - **Leave-one-out across datasets (5-fold)**: for each held-out dataset D, train on 100% of the other two datasets (2000 samples total) and evaluate on all 5 folds of D; aggregate mean Â± std.
   - **Global model (5-fold)**: for each fold, train one model on the union of all datasets' training splits (2400 samples = 3 datasets Ã— 800) and evaluate on each dataset's test split; aggregate mean Â± std per dataset.
6. **Persist artifacts** under `runs/<timestamp>/`, including:
   - `cross_validation/` â€“ per-dataset 5-fold CV results.
   - `per_dataset_models/` â€“ single-split per-dataset and cross-dataset models.
   - `leave_one_out_models/` â€“ 5-fold leave-one-out metrics and confusion matrices.
   - `global_model/` â€“ 5-fold global model metrics and confusion matrices.
   - `overall_summary.json` â€“ consolidated JSON with all metrics for the run.

## ğŸ“¦ Key Outputs Per Run

### `python scripts/train.py`

- `runs/<timestamp>/cross_validation/`  
  Per-dataset CV (within-dataset performance)
  - `reddit_cv_results.json` - Reddit 5-fold CV results with fold-level metrics
  - `4chan_cv_results.json` - 4chan 5-fold CV results
  - `twitter_cv_results.json` - Twitter 5-fold CV results
  - `cv_summary.csv` - Aggregated results table with mean Â± std

- `runs/<timestamp>/per_dataset_models/`  
  Per-dataset models + single-split cross-dataset evaluation
  - `<dataset>_model.pkl` - model trained only on that dataset
  - `<dataset>_model_eval_on_<other>.json` - cross-dataset metrics
  - `cross_dataset_results.json` - nested summary of trainâ†’test combinations

- `runs/<timestamp>/leave_one_out_models/`  
  Leave-one-out across datasets with 5-fold CV on the held-out dataset
  - `all_except_<held_out>_foldk_model.pkl` - models trained on the other datasets
  - `metrics_all_except_<held_out>_foldk_on_<held_out>.json` - per-fold metrics
  - `metrics_all_except_<held_out>_on_<held_out>.json` - aggregated mean Â± std
  - `leave_one_out_summary.json` - overall summary

- `runs/<timestamp>/global_model/`  
  Global model (train on all datasets jointly, 5-fold CV per dataset)
  - `global_model.pkl` - representative global model
  - `metrics_global_foldk_on_<dataset>.json` - per-fold metrics
  - `metrics_global_on_<dataset>.json` - aggregated mean Â± std per dataset
  - `global_model_summary.json` - overall summary

- `runs/<timestamp>/overall_summary.json`  
  Single JSON file aggregating:
  - `cross_validation`, `per_dataset`, `leave_one_out`, `global` results

### `python scripts/feature_group_comparison.py`

- `runs/feature_group_comparison_<timestamp>/`
  - `<dataset>_results.csv` for each platform
  - `all_datasets_results.csv` (combined metrics table)
  - Ablation plots: `f1_comparison_all_experiments.png`, `delta_f1_comparison.png`, `f1_heatmap.png`, `feature_group_rankings.png`
  - **Model**: Uses Logistic Regression classifier (class_weight='balanced', solver='saga', max_iter=1000) for feature ablation studies
  - **Experiments**: 11 experiments per dataset (full model, single-group-only, leave-one-group-out ablations)

## ğŸ“Š Feature Breakdown

| Group | Count | Description |
|-------|-------|-------------|
| **Lexical** | 14 | Word count, avg length, uppercase/exclamation/question/period ratios, punctuation counts, char patterns, word patterns, readability index |
| **Hate Lexicon** | 3 | Hate word count, ratio, binary flag (based on fixed curated `hate_lexicon.txt`) |
| **TF-IDF** | 1000 | Selected unigrams & bigrams (from up to 4000) |
| **Sentiment** | 10 | Polarity, subjectivity, magnitude, polarity/subjectivity category flags |
| **Embedding-based** | 390+ | 384D sentence embeddings, POS ratios, pronoun ratios, lexicon similarity, embedding distance |
| **Total** | **~1417** | Down from ~4400 raw features |

## ğŸ“„ License

This codebase is released under the MIT License. Make sure any datasets you add comply with their original licenses.

---

## ğŸ‘¥ Project Information

**Machine Learning Project - Technion**

**Creators:**
- Kamil Bokaae
- Najeeb Abu-Kheit

**Supervisors:**
- Shaul Markovitch
- Noam Gavish
